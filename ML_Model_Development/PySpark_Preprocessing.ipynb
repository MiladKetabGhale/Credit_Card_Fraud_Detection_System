{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, log1p\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# ML imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 1. Initialize Spark\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OptimizedStratifiedSplittingWithPreprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 2. Load and Shuffle the Dataset\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "DATA_PATH = \"creditcard.csv\"\n",
    "\n",
    "df = spark.read.csv(DATA_PATH, header=True, inferSchema=True).sample(fraction=1.0, seed=42)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 3. Split Data into Train, Validation, and Test\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "train_fraction = 0.8\n",
    "val_fraction   = 0.1\n",
    "test_fraction  = 0.1\n",
    "\n",
    "splits = df.randomSplit([train_fraction, val_fraction, test_fraction], seed=42)\n",
    "train_set, val_set, test_set = splits[0], splits[1], splits[2]\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 4. Utility Functions\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def clean_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Drop duplicates and rows with nulls.\"\"\"\n",
    "    return df.dropDuplicates().dropna()\n",
    "\n",
    "def log_normalize(df: DataFrame, column: str) -> DataFrame:\n",
    "    \"\"\"Apply log1p(x) to a given column (e.g., 'Amount').\"\"\"\n",
    "    return df.withColumn(column, log1p(col(column)))\n",
    "\n",
    "def build_preprocessing_pipeline(feature_cols):\n",
    "    \"\"\"\n",
    "    Build a Spark ML Pipeline consisting of:\n",
    "     1. VectorAssembler  -> to combine features into a single 'features' column\n",
    "     2. StandardScaler   -> to scale/standardize the 'features'\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    return pipeline\n",
    "\n",
    "def explode_scaled_features(df: DataFrame, feature_cols) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert 'scaled_features' (Vector) into a Spark array, then\n",
    "    create columns named exactly the same as feature_cols (e.g. \"V1\", \"V2\", ...).\n",
    "\n",
    "    IMPORTANT: We first drop the old columns \"V1..V28\" so that we don't\n",
    "    get a 'COLUMN_ALREADY_EXISTS' error when we create new columns\n",
    "    with those same names.\n",
    "    \"\"\"\n",
    "    # 1. Drop the original unscaled feature columns.\n",
    "    for c in feature_cols:\n",
    "        df = df.drop(c)\n",
    "\n",
    "    # 2. Convert ML Vector to Spark Array\n",
    "    df = df.withColumn(\"scaled_array\", vector_to_array(col(\"scaled_features\")))\n",
    "\n",
    "    # 3. Build the list of columns to SELECT\n",
    "    select_exprs = [\n",
    "        col(c) for c in df.columns\n",
    "        if c not in {\"scaled_features\", \"scaled_array\", \"features\"}\n",
    "    ]\n",
    "    # 4. Add each scaled feature as a new column with the old name\n",
    "    select_exprs += [\n",
    "        col(\"scaled_array\")[i].alias(feature_cols[i])\n",
    "        for i in range(len(feature_cols))\n",
    "    ]\n",
    "\n",
    "    # 5. Final select\n",
    "    df = df.select(*select_exprs)\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 5. Main Preprocessing / Fitting Flow\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def preprocess_dataset(df: DataFrame, pipeline_model, feature_cols, name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess a dataset:\n",
    "      - Clean (drop duplicates, na)\n",
    "      - Log-normalize 'Amount'\n",
    "      - Transform via fitted pipeline\n",
    "      - Explode scaled features\n",
    "    \"\"\"\n",
    "    print(f\"Preprocessing {name} dataset...\")\n",
    "    cleaned = clean_data(df)\n",
    "    normalized = log_normalize(cleaned, \"Amount\")\n",
    "    transformed = pipeline_model.transform(normalized)\n",
    "    final_df = explode_scaled_features(transformed, feature_cols)\n",
    "    return final_df\n",
    "\n",
    "def preprocess_multiple_datasets(datasets: dict, pipeline_model, feature_cols) -> dict:\n",
    "    \"\"\"\n",
    "    Apply 'preprocess_dataset' to each dataset in the dictionary.\n",
    "    \"\"\"\n",
    "    preprocessed_datasets = {}\n",
    "    for name, df in datasets.items():\n",
    "        preprocessed_datasets[name] = preprocess_dataset(df, pipeline_model, feature_cols, name)\n",
    "    return preprocessed_datasets\n",
    "\n",
    "def fit_and_transform_data(train_df: DataFrame, val_df: DataFrame, test_df: DataFrame):\n",
    "    \"\"\"\n",
    "    1. Clean & log-normalize the training set, then fit the pipeline.\n",
    "    2. Transform train, validation, and test sets.\n",
    "    3. Return the preprocessed DataFrames.\n",
    "    \"\"\"\n",
    "    pca_features = [f\"V{i}\" for i in range(1, 29)]\n",
    "\n",
    "    # -- Fit the pipeline on the TRAIN set --\n",
    "    train_cleaned = clean_data(train_df)\n",
    "    train_cleaned = log_normalize(train_cleaned, \"Amount\")\n",
    "\n",
    "    pipeline = build_preprocessing_pipeline(pca_features)\n",
    "    pipeline_model = pipeline.fit(train_cleaned)\n",
    "\n",
    "    # -- Transform all splits with the fitted model --\n",
    "    datasets = {\n",
    "        \"train\": train_df,\n",
    "        \"val\": val_df,\n",
    "        \"test\": test_df\n",
    "    }\n",
    "    preprocessed = preprocess_multiple_datasets(datasets, pipeline_model, pca_features)\n",
    "\n",
    "    return preprocessed[\"train\"], preprocessed[\"val\"], preprocessed[\"test\"]\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 6. Splitting Out Labels (Class) and Saving to CSV\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def save_features_and_labels(df: DataFrame, label_col: str, features_path: str, labels_path: str):\n",
    "    \"\"\"\n",
    "    Given a DataFrame that still contains the label column:\n",
    "      1. Split into features_df (everything except label_col)\n",
    "      2. Split into labels_df (only label_col)\n",
    "      3. Save each to CSV\n",
    "    \"\"\"\n",
    "    # 1) Labels only\n",
    "    labels_df = df.select(label_col)\n",
    "    # 2) Features only\n",
    "    features_df = df.drop(label_col)\n",
    "\n",
    "    # Save to CSV\n",
    "    features_df.coalesce(1).write.csv(features_path, header=True, mode=\"overwrite\")\n",
    "    labels_df.coalesce(1).write.csv(labels_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "def save_all_splits(\n",
    "    train_df: DataFrame, val_df: DataFrame, test_df: DataFrame, label_col: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Save train, val, and test splits as separate features+labels CSV files.\n",
    "    \"\"\"\n",
    "    # You can of course rename the paths as you wish\n",
    "    TRAIN_FEATURES_PATH = \"preprocessed_creditcard_train_features\"\n",
    "    TRAIN_LABELS_PATH   = \"preprocessed_creditcard_train_labels\"\n",
    "\n",
    "    VAL_FEATURES_PATH   = \"preprocessed_creditcard_validation_features\"\n",
    "    VAL_LABELS_PATH     = \"preprocessed_creditcard_validation_labels\"\n",
    "\n",
    "    TEST_FEATURES_PATH  = \"preprocessed_creditcard_test_features\"\n",
    "    TEST_LABELS_PATH    = \"preprocessed_creditcard_test_labels\"\n",
    "\n",
    "    # -- Train --\n",
    "    save_features_and_labels(train_df, label_col, TRAIN_FEATURES_PATH, TRAIN_LABELS_PATH)\n",
    "    # -- Validation --\n",
    "    save_features_and_labels(val_df, label_col, VAL_FEATURES_PATH, VAL_LABELS_PATH)\n",
    "    # -- Test --\n",
    "    save_features_and_labels(test_df, label_col, TEST_FEATURES_PATH, TEST_LABELS_PATH)\n",
    "\n",
    "    print(\"Preprocessed data splits (features + labels) saved to CSV:\")\n",
    "    print(f\"  Train features: {TRAIN_FEATURES_PATH}, Train labels: {TRAIN_LABELS_PATH}\")\n",
    "    print(f\"  Val features:   {VAL_FEATURES_PATH}, Val labels:   {VAL_LABELS_PATH}\")\n",
    "    print(f\"  Test features:  {TEST_FEATURES_PATH}, Test labels:  {TEST_LABELS_PATH}\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 7. Putting It All Together\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 7.1 Fit the pipeline on the train set, transform all splits\n",
    "train_preprocessed, val_preprocessed, test_preprocessed = fit_and_transform_data(\n",
    "    train_set, val_set, test_set\n",
    ")\n",
    "\n",
    "# 7.2 Save Features and Labels Separately\n",
    "#     (We assume the label is in a column named \"Class\")\n",
    "save_all_splits(train_preprocessed, val_preprocessed, test_preprocessed, label_col=\"Class\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Preprocessing + saving completed in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_csv_files(parent_directory):\n",
    "    \"\"\"\n",
    "    Navigate through subdirectories in the parent directory, find the CSV file\n",
    "    starting with 'part', and rename it to match the name of the subdirectory.\n",
    "\n",
    "    Args:\n",
    "        parent_directory (str): Path to the parent directory containing subdirectories.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Iterate through all items in the parent directory\n",
    "    for subdirectory in os.listdir(parent_directory):\n",
    "        # Construct full path for each subdirectory\n",
    "        subdirectory_path = os.path.join(parent_directory, subdirectory)\n",
    "        \n",
    "        # Check if the path is a directory\n",
    "        if os.path.isdir(subdirectory_path):\n",
    "            # Look for files in the subdirectory\n",
    "            for file_name in os.listdir(subdirectory_path):\n",
    "                if file_name.startswith(\"part\") and file_name.endswith(\".csv\"):\n",
    "                    # Construct old and new file paths\n",
    "                    old_file_path = os.path.join(subdirectory_path, file_name)\n",
    "                    new_file_name = f\"{subdirectory}.csv\"\n",
    "                    new_file_path = os.path.join(subdirectory_path, new_file_name)\n",
    "                    \n",
    "                    # Rename the file\n",
    "                    os.rename(old_file_path, new_file_path)\n",
    "                    print(f\"Renamed '{old_file_path}' to '{new_file_path}'\")\n",
    "                    break  # Only one file should exist; no need to check further\n",
    "\n",
    "# Example Usage\n",
    "parent_dir = \".\"  # Replace with your actual parent directory path\n",
    "rename_csv_files(parent_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
