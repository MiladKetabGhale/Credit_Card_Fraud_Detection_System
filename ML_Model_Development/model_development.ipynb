{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Class| count|\n",
      "+-----+------+\n",
      "|    1|   492|\n",
      "|    0|284315|\n",
      "+-----+------+\n",
      "\n",
      "Stratified split execution time: 0.18 seconds\n",
      "+-----+------+\n",
      "|Class| count|\n",
      "+-----+------+\n",
      "|    1|   296|\n",
      "|    0|181249|\n",
      "+-----+------+\n",
      "\n",
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|    1|   78|\n",
      "|    0|44882|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|    1|  100|\n",
      "|    0|57183|\n",
      "+-----+-----+\n",
      "\n",
      "Data splits saved to CSV files:\n",
      "Train set: creditcard_train.csv\n",
      "Validation set: creditcard_validation.csv\n",
      "Test set: creditcard_test.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OptimizedStratifiedSplitting\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Parameters for file paths\n",
    "DATA_PATH = \"creditcard.csv\"\n",
    "TRAIN_PATH = \"creditcard_train.csv\"\n",
    "VAL_PATH = \"creditcard_validation.csv\"\n",
    "TEST_PATH = \"creditcard_test.csv\"\n",
    "\n",
    "# Load and shuffle the dataset\n",
    "df = spark.read.csv(DATA_PATH, header=True, inferSchema=True).sample(fraction=1.0, seed=42)\n",
    "\n",
    "# Check class distribution\n",
    "class_distribution = df.groupBy(\"Class\").count()\n",
    "class_distribution.show()\n",
    "\n",
    "def stratified_split(df: DataFrame, strat_col: str, test_frac: float, val_frac: float, seed: int):\n",
    "    # Get class counts once\n",
    "    class_counts = {row[strat_col]: row[\"count\"] for row in df.groupBy(strat_col).count().collect()}\n",
    "    \n",
    "    # Define fractions for test split based on class distribution\n",
    "    fractions = {key: test_frac for key in class_counts.keys()}\n",
    "    \n",
    "    # Sample test set using sampleBy\n",
    "    test_set = df.sampleBy(strat_col, fractions, seed)\n",
    "    \n",
    "    # Remove test set from original data\n",
    "    train_val_set = df.subtract(test_set)\n",
    "    \n",
    "    # Define fractions for train and validation split\n",
    "    train_frac = 1 - val_frac\n",
    "    train_val_fractions = {key: train_frac for key in class_counts.keys()}\n",
    "    \n",
    "    # Sample train set from remaining data using sampleBy\n",
    "    train_set = train_val_set.sampleBy(strat_col, train_val_fractions, seed)\n",
    "    \n",
    "    # Remaining rows in train_val_set become validation set\n",
    "    val_set = train_val_set.subtract(train_set)\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "\n",
    "# Apply stratified split\n",
    "train_set, val_set, test_set = stratified_split(df, strat_col=\"Class\", test_frac=0.2, val_frac=0.2, seed=42)\n",
    "\n",
    "# Verify splits\n",
    "##train_set.groupBy(\"Class\").count().show()\n",
    "##val_set.groupBy(\"Class\").count().show()\n",
    "##test_set.groupBy(\"Class\").count().show()\n",
    "\n",
    "# Write splits to CSV files\n",
    "##train_set.coalesce(1).write.csv(TRAIN_PATH, header=True, mode=\"overwrite\")\n",
    "##val_set.coalesce(1).write.csv(VAL_PATH, header=True, mode=\"overwrite\")\n",
    "##test_set.coalesce(1).write.csv(TEST_PATH, header=True, mode=\"overwrite\")\n",
    "\n",
    "##print(\"Data splits saved to CSV files:\")\n",
    "##print(f\"Train set: {TRAIN_PATH}\")\n",
    "##print(f\"Validation set: {VAL_PATH}\")\n",
    "##print(f\"Test set: {TEST_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, log1p, mean, stddev\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PreprocessingPipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Parameters for file paths\n",
    "DATA_PATH = \"creditcard.csv\"\n",
    "PREPROCESSED_PATH = \"creditcard_preprocessed.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv(DATA_PATH, header=True, inferSchema=True)\n",
    "\n",
    "# Function to drop duplicates and null values\n",
    "def clean_data(df: DataFrame) -> DataFrame:\n",
    "    return df.dropDuplicates().dropna()\n",
    "\n",
    "# Function to apply log normalization to the 'Amount' column\n",
    "def log_normalize(df: DataFrame, column: str) -> DataFrame:\n",
    "    return df.withColumn(column, log1p(col(column)))\n",
    "\n",
    "# Function to standardize PCA-transformed features\n",
    "def standardize_features(df: DataFrame, feature_columns: list) -> DataFrame:\n",
    "    for col_name in feature_columns:\n",
    "        stats = df.select(mean(col(col_name)).alias(\"mean\"), stddev(col(col_name)).alias(\"stddev\")).collect()[0]\n",
    "        mean_val = stats[\"mean\"]\n",
    "        stddev_val = stats[\"stddev\"]\n",
    "\n",
    "        if stddev_val != 0:  # Avoid division by zero\n",
    "            df = df.withColumn(col_name, (col(col_name) - mean_val) / stddev_val)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocessing pipeline\n",
    "def preprocessing_pipeline(df: DataFrame) -> DataFrame:\n",
    "    # Step 1: Clean data (drop duplicates and null values)\n",
    "    df = clean_data(df)\n",
    "\n",
    "    # Step 2: Apply log normalization to the 'Amount' column\n",
    "    df = log_normalize(df, \"Amount\")\n",
    "\n",
    "    # Step 3: Standardize PCA-transformed features\n",
    "    pca_features = [f\"V{i}\" for i in range(1, 29)]  # V1 to V28\n",
    "    df = standardize_features(df, pca_features)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the preprocessing pipeline\n",
    "df_preprocessed = preprocessing_pipeline(df)\n",
    "\n",
    "# Write the preprocessed data to a CSV file\n",
    "df_preprocessed.coalesce(1).write.csv(PREPROCESSED_PATH, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Preprocessed data saved to: {PREPROCESSED_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Class| count|\n",
      "+-----+------+\n",
      "|    1|   492|\n",
      "|    0|284315|\n",
      "+-----+------+\n",
      "\n",
      "Preprocessed data splits saved to CSV files:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PREPROCESSED_TRAIN_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cb653e1a50ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preprocessed data splits saved to CSV files:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train set: {PREPROCESSED_TRAIN_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation set: {PREPROCESSED_VAL_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test set: {PREPROCESSED_TEST_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PREPROCESSED_TRAIN_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, log1p, mean, stddev\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OptimizedStratifiedSplitting\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Parameters for file paths\n",
    "DATA_PATH = \"creditcard.csv\"\n",
    "TRAIN_PATH = \"creditcard_train\"\n",
    "VAL_PATH = \"creditcard_validation\"\n",
    "TEST_PATH = \"creditcard_test\"\n",
    "\n",
    "# Load and shuffle the dataset\n",
    "df = spark.read.csv(DATA_PATH, header=True, inferSchema=True).sample(fraction=1.0, seed=42)\n",
    "\n",
    "# Check class distribution\n",
    "class_distribution = df.groupBy(\"Class\").count()\n",
    "class_distribution.show()\n",
    "\n",
    "def stratified_split(df: DataFrame, strat_col: str, test_frac: float, val_frac: float, seed: int):\n",
    "    # Get class counts once\n",
    "    class_counts = {row[strat_col]: row[\"count\"] for row in df.groupBy(strat_col).count().collect()}\n",
    "    \n",
    "    # Define fractions for test split based on class distribution\n",
    "    fractions = {key: test_frac for key in class_counts.keys()}\n",
    "    \n",
    "    # Sample test set using sampleBy\n",
    "    test_set = df.sampleBy(strat_col, fractions, seed)\n",
    "    \n",
    "    # Remove test set from original data\n",
    "    train_val_set = df.subtract(test_set)\n",
    "    \n",
    "    # Define fractions for train and validation split\n",
    "    train_frac = 1 - val_frac\n",
    "    train_val_fractions = {key: train_frac for key in class_counts.keys()}\n",
    "    \n",
    "    # Sample train set from remaining data using sampleBy\n",
    "    train_set = train_val_set.sampleBy(strat_col, train_val_fractions, seed)\n",
    "    \n",
    "    # Remaining rows in train_val_set become validation set\n",
    "    val_set = train_val_set.subtract(train_set)\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "# Preprocessing pipeline functions\n",
    "def clean_data(df: DataFrame) -> DataFrame:\n",
    "    return df.dropDuplicates().dropna()\n",
    "\n",
    "def log_normalize(df: DataFrame, column: str) -> DataFrame:\n",
    "    return df.withColumn(column, log1p(col(column)))\n",
    "\n",
    "def standardize_features(df: DataFrame, feature_columns: list) -> DataFrame:\n",
    "    for col_name in feature_columns:\n",
    "        stats = df.select(mean(col(col_name)).alias(\"mean\"), stddev(col(col_name)).alias(\"stddev\")).collect()[0]\n",
    "        mean_val = stats[\"mean\"]\n",
    "        stddev_val = stats[\"stddev\"]\n",
    "\n",
    "        if stddev_val != 0:  # Avoid division by zero\n",
    "            df = df.withColumn(col_name, (col(col_name) - mean_val) / stddev_val)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocessing_pipeline(df: DataFrame) -> DataFrame:\n",
    "    df = clean_data(df)\n",
    "    df = log_normalize(df, \"Amount\")\n",
    "    pca_features = [f\"V{i}\" for i in range(1, 29)]  # V1 to V28\n",
    "    df = standardize_features(df, pca_features)\n",
    "    return df\n",
    "\n",
    "# Apply stratified split\n",
    "train_set, val_set, test_set = stratified_split(df, strat_col=\"Class\", test_frac=0.2, val_frac=0.2, seed=42)\n",
    "\n",
    "# Apply preprocessing pipeline and save outputs\n",
    "preprocessed_train_set = preprocessing_pipeline(train_set)\n",
    "preprocessed_val_set = preprocessing_pipeline(val_set)\n",
    "preprocessed_test_set = preprocessing_pipeline(test_set)\n",
    "\n",
    "# Save preprocessed data\n",
    "preprocessed_train_set.coalesce(1).write.csv(TRAIN_PATH, header=True, mode=\"overwrite\")\n",
    "preprocessed_val_set.coalesce(1).write.csv(VAL_PATH, header=True, mode=\"overwrite\")\n",
    "preprocessed_test_set.coalesce(1).write.csv(TEST_PATH, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"Preprocessed data splits saved to CSV files:\")\n",
    "print(f\"Train set: {TRAIN_PATH}\")\n",
    "print(f\"Validation set: {VAL_PATH}\")\n",
    "print(f\"Test set: {TEST_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing and save completed in 7.51 seconds\n",
      "Preprocessed data splits saved to CSV files:\n",
      "  Train set: preprocessed_creditcard_train.csv\n",
      "  Validation set: preprocessed_creditcard_validation.csv\n",
      "  Test set: preprocessed_creditcard_test.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, log1p\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# ML imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 1. Initialize Spark\n",
    "# --------------------------------------------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OptimizedStratifiedSplittingWithPreprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 2. Load and Shuffle the Dataset\n",
    "# --------------------------------------------------------------------------------\n",
    "DATA_PATH = \"creditcard.csv\"\n",
    "\n",
    "df = spark.read.csv(DATA_PATH, header=True, inferSchema=True).sample(fraction=1.0, seed=42)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 3. Split Data into Train, Validation, and Test\n",
    "# --------------------------------------------------------------------------------\n",
    "train_fraction = 0.7\n",
    "val_fraction   = 0.15\n",
    "test_fraction  = 0.15\n",
    "\n",
    "splits = df.randomSplit([train_fraction, val_fraction, test_fraction], seed=42)\n",
    "train_set, val_set, test_set = splits[0], splits[1], splits[2]\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 4. Utility Functions\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def clean_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Drop duplicates and rows with nulls.\"\"\"\n",
    "    return df.dropDuplicates().dropna()\n",
    "\n",
    "def log_normalize(df: DataFrame, column: str) -> DataFrame:\n",
    "    \"\"\"Apply log1p(x) to a given column (e.g. Amount).\"\"\"\n",
    "    return df.withColumn(column, log1p(col(column)))\n",
    "\n",
    "def build_preprocessing_pipeline(feature_cols):\n",
    "    \"\"\"\n",
    "    Build a Spark ML Pipeline consisting of:\n",
    "     1) VectorAssembler  -> to combine features into a single 'features' column\n",
    "     2) StandardScaler   -> to scale/standardize the 'features'\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    return pipeline\n",
    "\n",
    "def explode_scaled_features(df: DataFrame, feature_cols) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert 'scaled_features' (Vector) into a Spark array, then\n",
    "    create columns named exactly the same as feature_cols (e.g. \"V1\", \"V2\", ...).\n",
    "    \n",
    "    IMPORTANT: We first drop the old columns \"V1..V28\" so that we don't\n",
    "    get a 'COLUMN_ALREADY_EXISTS' error when we create new columns\n",
    "    with those same names.\n",
    "    \"\"\"\n",
    "    # 1) Drop the original unscaled feature columns.\n",
    "    #    Otherwise, Spark complains that column X already exists.\n",
    "    for c in feature_cols:\n",
    "        df = df.drop(c)\n",
    "\n",
    "    # 2) Convert ML Vector to Spark Array\n",
    "    df = df.withColumn(\"scaled_array\", vector_to_array(col(\"scaled_features\")))\n",
    "\n",
    "    # 3) Build the list of columns to SELECT\n",
    "    #    (keep everything except the columns we no longer need)\n",
    "    select_exprs = [\n",
    "        col(c) for c in df.columns \n",
    "        if c not in {\"scaled_features\", \"scaled_array\", \"features\"}\n",
    "    ]\n",
    "\n",
    "    # 4) Add each scaled feature as a new column with the old name\n",
    "    select_exprs += [\n",
    "        col(\"scaled_array\")[i].alias(feature_cols[i]) \n",
    "        for i in range(len(feature_cols))\n",
    "    ]\n",
    "    \n",
    "    # 5) Final select\n",
    "    df = df.select(*select_exprs)\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 5. Main Preprocessing / Fitting Flow\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def fit_and_transform_data(train_df: DataFrame, val_df: DataFrame, test_df: DataFrame) -> (DataFrame, DataFrame, DataFrame):\n",
    "    \"\"\"\n",
    "    1) Clean & log-normalize the training set, then fit the pipeline (assembler + scaler).\n",
    "    2) Transform train, validation, and test sets with the fitted pipeline.\n",
    "    3) Explode 'scaled_features' back into columns (replacing V1..V28).\n",
    "    \"\"\"\n",
    "    pca_features = [f\"V{i}\" for i in range(1, 29)]\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Step A: Pre-clean/normalize the TRAIN set\n",
    "    # -----------------------------------------------\n",
    "    train_cleaned = clean_data(train_df)\n",
    "    train_cleaned = log_normalize(train_cleaned, \"Amount\")\n",
    "\n",
    "    # Build and fit the pipeline on train\n",
    "    pipeline = build_preprocessing_pipeline(pca_features)\n",
    "    pipeline_model = pipeline.fit(train_cleaned)\n",
    "\n",
    "    # Transform train set\n",
    "    train_transformed = pipeline_model.transform(train_cleaned)\n",
    "    train_transformed = explode_scaled_features(train_transformed, pca_features)\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Step B: Transform VALIDATION set\n",
    "    # -----------------------------------------------\n",
    "    val_cleaned = clean_data(val_df)\n",
    "    val_cleaned = log_normalize(val_cleaned, \"Amount\")\n",
    "    val_transformed = pipeline_model.transform(val_cleaned)\n",
    "    val_transformed = explode_scaled_features(val_transformed, pca_features)\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Step C: Transform TEST set\n",
    "    # -----------------------------------------------\n",
    "    test_cleaned = clean_data(test_df)\n",
    "    test_cleaned = log_normalize(test_cleaned, \"Amount\")\n",
    "    test_transformed = pipeline_model.transform(test_cleaned)\n",
    "    test_transformed = explode_scaled_features(test_transformed, pca_features)\n",
    "\n",
    "    return train_transformed, val_transformed, test_transformed\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 6. Save to CSV\n",
    "# --------------------------------------------------------------------------------\n",
    "def save_to_csv(df: DataFrame, output_path: str):\n",
    "    \"\"\"\n",
    "    Write Spark DataFrame to CSV (single file).\n",
    "    Using coalesce(1) for a single CSV output. \n",
    "    \"\"\"\n",
    "    df.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 7. Putting It All Together\n",
    "# --------------------------------------------------------------------------------\n",
    "TRAIN_PATH = \"preprocessed_creditcard_train.csv\"\n",
    "VAL_PATH   = \"preprocessed_creditcard_validation.csv\"\n",
    "TEST_PATH  = \"preprocessed_creditcard_test.csv\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 7.1 Fit the pipeline on the train set, transform all splits\n",
    "train_preprocessed, val_preprocessed, test_preprocessed = fit_and_transform_data(\n",
    "    train_set, val_set, test_set\n",
    ")\n",
    "\n",
    "# 7.2 (Optional) Save the results\n",
    "save_to_csv(train_preprocessed, TRAIN_PATH)\n",
    "save_to_csv(val_preprocessed, VAL_PATH)\n",
    "save_to_csv(test_preprocessed, TEST_PATH)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Preprocessing and save completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"Preprocessed data splits saved to CSV files:\")\n",
    "print(f\"  Train set: {TRAIN_PATH}\")\n",
    "print(f\"  Validation set: {VAL_PATH}\")\n",
    "print(f\"  Test set: {TEST_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train dataset...\n",
      "Preprocessing val dataset...\n",
      "Preprocessing test dataset...\n",
      "Preprocessing and save completed in 5.51 seconds\n",
      "Preprocessed data splits saved to CSV files:\n",
      "  Train set: preprocessed_creditcard_train.csv\n",
      "  Validation set: preprocessed_creditcard_validation.csv\n",
      "  Test set: preprocessed_creditcard_test.csv\n"
     ]
    }
   ],
   "source": [
    "############################################# cleaner version of Above ####################################\n",
    "\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, log1p\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# ML imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 1. Initialize Spark\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OptimizedStratifiedSplittingWithPreprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 2. Load and Shuffle the Dataset\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "DATA_PATH = \"creditcard.csv\"\n",
    "\n",
    "df = spark.read.csv(DATA_PATH, header=True, inferSchema=True).sample(fraction=1.0, seed=42)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 3. Split Data into Train, Validation, and Test\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "train_fraction = 0.7\n",
    "val_fraction   = 0.15\n",
    "test_fraction  = 0.15\n",
    "\n",
    "splits = df.randomSplit([train_fraction, val_fraction, test_fraction], seed=42)\n",
    "train_set, val_set, test_set = splits[0], splits[1], splits[2]\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 4. Utility Functions\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def clean_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Drop duplicates and rows with nulls.\"\"\"\n",
    "    return df.dropDuplicates().dropna()\n",
    "\n",
    "def log_normalize(df: DataFrame, column: str) -> DataFrame:\n",
    "    \"\"\"Apply log1p(x) to a given column (e.g. Amount).\"\"\"\n",
    "    return df.withColumn(column, log1p(col(column)))\n",
    "\n",
    "def build_preprocessing_pipeline(feature_cols):\n",
    "    \"\"\"\n",
    "    Build a Spark ML Pipeline consisting of:\n",
    "     1. VectorAssembler  -> to combine features into a single 'features' column\n",
    "     2. StandardScaler   -> to scale/standardize the 'features'\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    return pipeline\n",
    "\n",
    "def explode_scaled_features(df: DataFrame, feature_cols) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert 'scaled_features' (Vector) into a Spark array, then\n",
    "    create columns named exactly the same as feature_cols (e.g. \"V1\", \"V2\", ...).\n",
    "    \n",
    "    IMPORTANT: We first drop the old columns \"V1..V28\" so that we don't\n",
    "    get a 'COLUMN_ALREADY_EXISTS' error when we create new columns\n",
    "    with those same names.\n",
    "    \"\"\"\n",
    "    # 1. Drop the original unscaled feature columns.\n",
    "    #    Otherwise, Spark complains that column X already exists.\n",
    "    for c in feature_cols:\n",
    "        df = df.drop(c)\n",
    "\n",
    "    # 2. Convert ML Vector to Spark Array\n",
    "    df = df.withColumn(\"scaled_array\", vector_to_array(col(\"scaled_features\")))\n",
    "\n",
    "    # 3. Build the list of columns to SELECT\n",
    "    #    (keep everything except the columns we no longer need)\n",
    "    select_exprs = [\n",
    "        col(c) for c in df.columns \n",
    "        if c not in {\"scaled_features\", \"scaled_array\", \"features\"}\n",
    "    ]\n",
    "\n",
    "    # 4. Add each scaled feature as a new column with the old name\n",
    "    select_exprs += [\n",
    "        col(\"scaled_array\")[i].alias(feature_cols[i]) \n",
    "        for i in range(len(feature_cols))\n",
    "    ]\n",
    "    \n",
    "    # 5. Final select\n",
    "    df = df.select(*select_exprs)\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 5. Main Preprocessing / Fitting Flow\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def preprocess_dataset(df: DataFrame, pipeline_model, feature_cols, name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Generalized function to preprocess a dataset.\n",
    "    Performs cleaning, log-normalization, pipeline transformation, \n",
    "    and exploding scaled features into separate columns.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input Spark DataFrame (e.g., train, val, or test).\n",
    "        pipeline_model: Fitted pipeline model for transforming the dataset.\n",
    "        feature_cols (list): List of feature column names (e.g., V1 to V28).\n",
    "        name (str): Name of the dataset (train/val/test) for logging.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Preprocessed Spark DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"Preprocessing {name} dataset...\")\n",
    "    cleaned = clean_data(df)\n",
    "    normalized = log_normalize(cleaned, \"Amount\")\n",
    "    transformed = pipeline_model.transform(normalized)\n",
    "    final_df = explode_scaled_features(transformed, feature_cols)\n",
    "    return final_df\n",
    "\n",
    "def preprocess_multiple_datasets(datasets: dict, pipeline_model, feature_cols) -> dict:\n",
    "    \"\"\"\n",
    "    Preprocess multiple datasets by calling preprocess_dataset for each.\n",
    "\n",
    "    Args:\n",
    "        datasets (dict): A dictionary where keys are dataset names (e.g., \"train\", \"val\", \"test\")\n",
    "                         and values are the corresponding DataFrames.\n",
    "        pipeline_model: Fitted pipeline model for transforming the datasets.\n",
    "        feature_cols (list): List of feature column names (e.g., V1 to V28).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are dataset names and values are preprocessed DataFrames.\n",
    "    \"\"\"\n",
    "    preprocessed_datasets = {}\n",
    "    for name, df in datasets.items():\n",
    "        preprocessed_datasets[name] = preprocess_dataset(df, pipeline_model, feature_cols, name)\n",
    "    return preprocessed_datasets\n",
    "\n",
    "\n",
    "def fit_and_transform_data(train_df: DataFrame, val_df: DataFrame, test_df: DataFrame) -> (DataFrame, DataFrame, DataFrame):\n",
    "    \"\"\"\n",
    "    1. Clean & log-normalize the training set, then fit the pipeline (assembler + scaler).\n",
    "    2. Use a general preprocessing function for train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    pca_features = [f\"V{i}\" for i in range(1, 29)]\n",
    "\n",
    "    # Preprocess the training set\n",
    "    train_cleaned = clean_data(train_df)\n",
    "    train_cleaned = log_normalize(train_cleaned, \"Amount\")\n",
    "\n",
    "    # Build and fit the pipeline on train\n",
    "    pipeline = build_preprocessing_pipeline(pca_features)\n",
    "    pipeline_model = pipeline.fit(train_cleaned)\n",
    "\n",
    "    # Define datasets\n",
    "    datasets = {\n",
    "        \"train\": train_df,\n",
    "        \"val\": val_df,\n",
    "        \"test\": test_df\n",
    "    }\n",
    "\n",
    "    # Preprocess all datasets\n",
    "    preprocessed = preprocess_multiple_datasets(datasets, pipeline_model, pca_features)\n",
    "\n",
    "    return preprocessed[\"train\"], preprocessed[\"val\"], preprocessed[\"test\"]\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 6. Save to CSV\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def save_to_csv(df: DataFrame, output_path: str):\n",
    "    \"\"\"\n",
    "    Write Spark DataFrame to CSV (single file).\n",
    "    Using coalesce(1) for a single CSV output. \n",
    "    \"\"\"\n",
    "    df.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 7. Putting It All Together\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "TRAIN_PATH = \"preprocessed_creditcard_train.csv\"\n",
    "VAL_PATH   = \"preprocessed_creditcard_validation.csv\"\n",
    "TEST_PATH  = \"preprocessed_creditcard_test.csv\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 7.1 Fit the pipeline on the train set, transform all splits\n",
    "train_preprocessed, val_preprocessed, test_preprocessed = fit_and_transform_data(\n",
    "    train_set, val_set, test_set\n",
    ")\n",
    "\n",
    "# 7.2 (Optional) Save the results\n",
    "save_to_csv(train_preprocessed, TRAIN_PATH)\n",
    "save_to_csv(val_preprocessed, VAL_PATH)\n",
    "save_to_csv(test_preprocessed, TEST_PATH)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Preprocessing and save completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"Preprocessed data splits saved to CSV files:\")\n",
    "print(f\"  Train set: {TRAIN_PATH}\")\n",
    "print(f\"  Validation set: {VAL_PATH}\")\n",
    "print(f\"  Test set: {TEST_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train dataset...\n",
      "Preprocessing val dataset...\n",
      "Preprocessing test dataset...\n",
      "Preprocessed data splits (features + labels) saved to CSV:\n",
      "  Train features: preprocessed_creditcard_train_features, Train labels: preprocessed_creditcard_train_labels\n",
      "  Val features:   preprocessed_creditcard_validation_features, Val labels:   preprocessed_creditcard_validation_labels\n",
      "  Test features:  preprocessed_creditcard_test_features, Test labels:  preprocessed_creditcard_test_labels\n",
      "Preprocessing + saving completed in 10.06 seconds\n"
     ]
    }
   ],
   "source": [
    "############################ this version separates out the labels for each set: train, val, and test ############\n",
    "\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, log1p\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# ML imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 1. Initialize Spark\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OptimizedStratifiedSplittingWithPreprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 2. Load and Shuffle the Dataset\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "DATA_PATH = \"creditcard.csv\"\n",
    "\n",
    "df = spark.read.csv(DATA_PATH, header=True, inferSchema=True).sample(fraction=1.0, seed=42)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 3. Split Data into Train, Validation, and Test\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "train_fraction = 0.8\n",
    "val_fraction   = 0.1\n",
    "test_fraction  = 0.1\n",
    "\n",
    "splits = df.randomSplit([train_fraction, val_fraction, test_fraction], seed=42)\n",
    "train_set, val_set, test_set = splits[0], splits[1], splits[2]\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 4. Utility Functions\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def clean_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Drop duplicates and rows with nulls.\"\"\"\n",
    "    return df.dropDuplicates().dropna()\n",
    "\n",
    "def log_normalize(df: DataFrame, column: str) -> DataFrame:\n",
    "    \"\"\"Apply log1p(x) to a given column (e.g., 'Amount').\"\"\"\n",
    "    return df.withColumn(column, log1p(col(column)))\n",
    "\n",
    "def build_preprocessing_pipeline(feature_cols):\n",
    "    \"\"\"\n",
    "    Build a Spark ML Pipeline consisting of:\n",
    "     1. VectorAssembler  -> to combine features into a single 'features' column\n",
    "     2. StandardScaler   -> to scale/standardize the 'features'\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    return pipeline\n",
    "\n",
    "def explode_scaled_features(df: DataFrame, feature_cols) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert 'scaled_features' (Vector) into a Spark array, then\n",
    "    create columns named exactly the same as feature_cols (e.g. \"V1\", \"V2\", ...).\n",
    "\n",
    "    IMPORTANT: We first drop the old columns \"V1..V28\" so that we don't\n",
    "    get a 'COLUMN_ALREADY_EXISTS' error when we create new columns\n",
    "    with those same names.\n",
    "    \"\"\"\n",
    "    # 1. Drop the original unscaled feature columns.\n",
    "    for c in feature_cols:\n",
    "        df = df.drop(c)\n",
    "\n",
    "    # 2. Convert ML Vector to Spark Array\n",
    "    df = df.withColumn(\"scaled_array\", vector_to_array(col(\"scaled_features\")))\n",
    "\n",
    "    # 3. Build the list of columns to SELECT\n",
    "    select_exprs = [\n",
    "        col(c) for c in df.columns\n",
    "        if c not in {\"scaled_features\", \"scaled_array\", \"features\"}\n",
    "    ]\n",
    "    # 4. Add each scaled feature as a new column with the old name\n",
    "    select_exprs += [\n",
    "        col(\"scaled_array\")[i].alias(feature_cols[i])\n",
    "        for i in range(len(feature_cols))\n",
    "    ]\n",
    "\n",
    "    # 5. Final select\n",
    "    df = df.select(*select_exprs)\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 5. Main Preprocessing / Fitting Flow\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def preprocess_dataset(df: DataFrame, pipeline_model, feature_cols, name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess a dataset:\n",
    "      - Clean (drop duplicates, na)\n",
    "      - Log-normalize 'Amount'\n",
    "      - Transform via fitted pipeline\n",
    "      - Explode scaled features\n",
    "    \"\"\"\n",
    "    print(f\"Preprocessing {name} dataset...\")\n",
    "    cleaned = clean_data(df)\n",
    "    normalized = log_normalize(cleaned, \"Amount\")\n",
    "    transformed = pipeline_model.transform(normalized)\n",
    "    final_df = explode_scaled_features(transformed, feature_cols)\n",
    "    return final_df\n",
    "\n",
    "def preprocess_multiple_datasets(datasets: dict, pipeline_model, feature_cols) -> dict:\n",
    "    \"\"\"\n",
    "    Apply 'preprocess_dataset' to each dataset in the dictionary.\n",
    "    \"\"\"\n",
    "    preprocessed_datasets = {}\n",
    "    for name, df in datasets.items():\n",
    "        preprocessed_datasets[name] = preprocess_dataset(df, pipeline_model, feature_cols, name)\n",
    "    return preprocessed_datasets\n",
    "\n",
    "def fit_and_transform_data(train_df: DataFrame, val_df: DataFrame, test_df: DataFrame):\n",
    "    \"\"\"\n",
    "    1. Clean & log-normalize the training set, then fit the pipeline.\n",
    "    2. Transform train, validation, and test sets.\n",
    "    3. Return the preprocessed DataFrames.\n",
    "    \"\"\"\n",
    "    pca_features = [f\"V{i}\" for i in range(1, 29)]\n",
    "\n",
    "    # -- Fit the pipeline on the TRAIN set --\n",
    "    train_cleaned = clean_data(train_df)\n",
    "    train_cleaned = log_normalize(train_cleaned, \"Amount\")\n",
    "\n",
    "    pipeline = build_preprocessing_pipeline(pca_features)\n",
    "    pipeline_model = pipeline.fit(train_cleaned)\n",
    "\n",
    "    # -- Transform all splits with the fitted model --\n",
    "    datasets = {\n",
    "        \"train\": train_df,\n",
    "        \"val\": val_df,\n",
    "        \"test\": test_df\n",
    "    }\n",
    "    preprocessed = preprocess_multiple_datasets(datasets, pipeline_model, pca_features)\n",
    "\n",
    "    return preprocessed[\"train\"], preprocessed[\"val\"], preprocessed[\"test\"]\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 6. Splitting Out Labels (Class) and Saving to CSV\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def save_features_and_labels(df: DataFrame, label_col: str, features_path: str, labels_path: str):\n",
    "    \"\"\"\n",
    "    Given a DataFrame that still contains the label column:\n",
    "      1. Split into features_df (everything except label_col)\n",
    "      2. Split into labels_df (only label_col)\n",
    "      3. Save each to CSV\n",
    "    \"\"\"\n",
    "    # 1) Labels only\n",
    "    labels_df = df.select(label_col)\n",
    "    # 2) Features only\n",
    "    features_df = df.drop(label_col)\n",
    "\n",
    "    # Save to CSV\n",
    "    features_df.coalesce(1).write.csv(features_path, header=True, mode=\"overwrite\")\n",
    "    labels_df.coalesce(1).write.csv(labels_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "def save_all_splits(\n",
    "    train_df: DataFrame, val_df: DataFrame, test_df: DataFrame, label_col: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Save train, val, and test splits as separate features+labels CSV files.\n",
    "    \"\"\"\n",
    "    # You can of course rename the paths as you wish\n",
    "    TRAIN_FEATURES_PATH = \"preprocessed_creditcard_train_features\"\n",
    "    TRAIN_LABELS_PATH   = \"preprocessed_creditcard_train_labels\"\n",
    "\n",
    "    VAL_FEATURES_PATH   = \"preprocessed_creditcard_validation_features\"\n",
    "    VAL_LABELS_PATH     = \"preprocessed_creditcard_validation_labels\"\n",
    "\n",
    "    TEST_FEATURES_PATH  = \"preprocessed_creditcard_test_features\"\n",
    "    TEST_LABELS_PATH    = \"preprocessed_creditcard_test_labels\"\n",
    "\n",
    "    # -- Train --\n",
    "    save_features_and_labels(train_df, label_col, TRAIN_FEATURES_PATH, TRAIN_LABELS_PATH)\n",
    "    # -- Validation --\n",
    "    save_features_and_labels(val_df, label_col, VAL_FEATURES_PATH, VAL_LABELS_PATH)\n",
    "    # -- Test --\n",
    "    save_features_and_labels(test_df, label_col, TEST_FEATURES_PATH, TEST_LABELS_PATH)\n",
    "\n",
    "    print(\"Preprocessed data splits (features + labels) saved to CSV:\")\n",
    "    print(f\"  Train features: {TRAIN_FEATURES_PATH}, Train labels: {TRAIN_LABELS_PATH}\")\n",
    "    print(f\"  Val features:   {VAL_FEATURES_PATH}, Val labels:   {VAL_LABELS_PATH}\")\n",
    "    print(f\"  Test features:  {TEST_FEATURES_PATH}, Test labels:  {TEST_LABELS_PATH}\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 7. Putting It All Together\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 7.1 Fit the pipeline on the train set, transform all splits\n",
    "train_preprocessed, val_preprocessed, test_preprocessed = fit_and_transform_data(\n",
    "    train_set, val_set, test_set\n",
    ")\n",
    "\n",
    "# 7.2 Save Features and Labels Separately\n",
    "#     (We assume the label is in a column named \"Class\")\n",
    "save_all_splits(train_preprocessed, val_preprocessed, test_preprocessed, label_col=\"Class\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Preprocessing + saving completed in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed './preprocessed_creditcard_validation_features/part-00000-e5780052-74ea-47be-99c3-0d55bd573bda-c000.csv' to './preprocessed_creditcard_validation_features/preprocessed_creditcard_validation_features.csv'\n",
      "Renamed './preprocessed_creditcard_test_labels/part-00000-440d9179-4251-4d05-8aff-0b6a65dd3e7b-c000.csv' to './preprocessed_creditcard_test_labels/preprocessed_creditcard_test_labels.csv'\n",
      "Renamed './preprocessed_creditcard_test_features/part-00000-4d58d280-c55e-4171-a1fe-e096bd30bc6f-c000.csv' to './preprocessed_creditcard_test_features/preprocessed_creditcard_test_features.csv'\n",
      "Renamed './preprocessed_creditcard_train_labels/part-00000-7e88f01c-73f0-4dd3-af50-2a273afef9fc-c000.csv' to './preprocessed_creditcard_train_labels/preprocessed_creditcard_train_labels.csv'\n",
      "Renamed './preprocessed_creditcard_train_features/part-00000-7056bdb4-10c9-40c5-b912-dddd19f66708-c000.csv' to './preprocessed_creditcard_train_features/preprocessed_creditcard_train_features.csv'\n",
      "Renamed './preprocessed_creditcard_validation_labels/part-00000-dc9a0a9d-b946-46d6-bd13-897cda57b7f5-c000.csv' to './preprocessed_creditcard_validation_labels/preprocessed_creditcard_validation_labels.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def rename_csv_files(parent_directory):\n",
    "    \"\"\"\n",
    "    Navigate through subdirectories in the parent directory, find the CSV file\n",
    "    starting with 'part', and rename it to match the name of the subdirectory.\n",
    "\n",
    "    Args:\n",
    "        parent_directory (str): Path to the parent directory containing subdirectories.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Iterate through all items in the parent directory\n",
    "    for subdirectory in os.listdir(parent_directory):\n",
    "        # Construct full path for each subdirectory\n",
    "        subdirectory_path = os.path.join(parent_directory, subdirectory)\n",
    "        \n",
    "        # Check if the path is a directory\n",
    "        if os.path.isdir(subdirectory_path):\n",
    "            # Look for files in the subdirectory\n",
    "            for file_name in os.listdir(subdirectory_path):\n",
    "                if file_name.startswith(\"part\") and file_name.endswith(\".csv\"):\n",
    "                    # Construct old and new file paths\n",
    "                    old_file_path = os.path.join(subdirectory_path, file_name)\n",
    "                    new_file_name = f\"{subdirectory}.csv\"\n",
    "                    new_file_path = os.path.join(subdirectory_path, new_file_name)\n",
    "                    \n",
    "                    # Rename the file\n",
    "                    os.rename(old_file_path, new_file_path)\n",
    "                    print(f\"Renamed '{old_file_path}' to '{new_file_path}'\")\n",
    "                    break  # Only one file should exist; no need to check further\n",
    "\n",
    "# Example Usage\n",
    "parent_dir = \".\"  # Replace with your actual parent directory path\n",
    "rename_csv_files(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_features = pd.read_csv(\"./preprocessed_creditcard_train_features/preprocessed_creditcard_train_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
